{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulationProperties = pd.read_csv('./ENB2012_data_f.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función para calcular la información mutua\n",
    "def calcular_informacion_mutua(x, y, bins_x=10, bins_y=10, epsilon=1e-10):\n",
    "    # Calcular los histogramas para cada variable\n",
    "    hist_x, edges_x = np.histogram(x, bins=bins_x, density=True)\n",
    "    hist_y, edges_y = np.histogram(y, bins=bins_y, density=True)\n",
    "\n",
    "    prob_X = hist_x * np.diff(edges_x)\n",
    "    prob_X[prob_X == 0] = epsilon\n",
    "    entropia_x = np.sum(prob_X * np.log2(prob_X))\n",
    "\n",
    "    # Calcular el histograma conjunto\n",
    "    hist_xy, _, _ = np.histogram2d(x, y, bins=[bins_x, bins_y], density=True)\n",
    "\n",
    "    # Calcular la probabilidad conjunta\n",
    "    prob_xy = hist_xy * np.diff(edges_x).reshape(-1, 1) * np.diff(edges_y)\n",
    "\n",
    "    # Evitar valores cero antes del logaritmo\n",
    "    prob_xy[prob_xy == 0] = epsilon\n",
    "\n",
    "    # Calcular la entropía conjunta\n",
    "    entropia_xy = np.sum(prob_xy * np.log2(prob_xy))\n",
    "  \n",
    "    # Calcular la información mutua\n",
    "    informacion_mutua = entropia_x - entropia_xy\n",
    "\n",
    "    return informacion_mutua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información Mutua: 1.7686540998425682\n",
      "Información Mutua Y1: 2.9897352904484364e-07\n",
      "1.7686540998425682\n"
     ]
    }
   ],
   "source": [
    "x = simulationProperties[\"X1\"]\n",
    "y = simulationProperties[\"Y1\"]\n",
    "info_mutua = calcular_informacion_mutua(x, y)\n",
    "info_mutua_y = calcular_informacion_mutua(y, y)\n",
    "print(\"Información Mutua:\", info_mutua)\n",
    "print(\"Información Mutua Y1:\", info_mutua_y)\n",
    "print(info_mutua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_entropy(x, y):\n",
    "    combined = list(zip(x, y))\n",
    "    counts = Counter(combined)\n",
    "    total = len(combined)\n",
    "    probabilities = [count / total for count in counts.values()]\n",
    "    entropy_value = -sum(p * np.log2(p) for p in probabilities)\n",
    "    \n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(series):\n",
    "    counts = Counter(series)\n",
    "    probabilities = [count / len(series) for count in counts.values()]\n",
    "    return -sum(p * np.log2(p) for p in probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im(x, y):\n",
    "  entropia_x = entropy(x)\n",
    "  print('entropia_x', entropia_x)\n",
    "  entropia_y = entropy(y)\n",
    "  entropy_x_y = joint_entropy(x, y)\n",
    "  return entropia_x + entropia_y - entropy_x_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(series) for count in counts.values() <generator object entropy.<locals>.<genexpr> at 0x1611a99a0>\n",
      "entropia_x 3.584962500721156\n",
      "len(series) for count in counts.values() <generator object entropy.<locals>.<genexpr> at 0x1611a99a0>\n",
      "len(series) for count in counts.values() <generator object entropy.<locals>.<genexpr> at 0x1611aac00>\n",
      "entropia_x 9.052740507837067\n",
      "len(series) for count in counts.values() <generator object entropy.<locals>.<genexpr> at 0x1611aac00>\n",
      "Información Mutua: 3.3591001595494454\n",
      "Información Mutua Y1: 9.052740507837067\n",
      "0.37105892482408304\n"
     ]
    }
   ],
   "source": [
    "x = simulationProperties[\"X1\"]\n",
    "y = simulationProperties[\"Y1\"]\n",
    "info_mutua = im(x, y)\n",
    "info_mutua_y = im(y, y)\n",
    "print(\"Información Mutua:\", info_mutua)\n",
    "print(\"Información Mutua Y1:\", info_mutua_y)\n",
    "print(info_mutua/info_mutua_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3283508048101482\n",
      "6.274881559347977\n",
      "0.3710589248240866\n"
     ]
    }
   ],
   "source": [
    "def prueba(x, y):\n",
    "    counts_x = Counter(x)\n",
    "    x_probabilities = {k: v / len(x) for k, v in counts_x.items()}\n",
    "    \n",
    "    counts_y = Counter(y)\n",
    "    y_probabilities = {k: v / len(y) for k, v in counts_y.items()}\n",
    "    \n",
    "    combined = list(zip(x, y))\n",
    "    counts = Counter(combined)\n",
    "    \n",
    "    join_probabilities = {k: v / len(combined) for k, v in counts.items()} \n",
    "    \n",
    "    sum_result = 0\n",
    "    for (xi, yi), p_xy in join_probabilities.items():\n",
    "        p_x_y = x_probabilities[xi] * y_probabilities[yi]\n",
    "        \n",
    "        # Comprobar que no haya división por cero antes de calcular el logaritmo\n",
    "        if p_x_y > 0:\n",
    "            sum_result += p_xy * np.log(p_xy / p_x_y)\n",
    "\n",
    "    return sum_result\n",
    "  \n",
    "value = prueba(x,y);\n",
    "value2 = prueba(y,y)\n",
    "print(value)\n",
    "print(value2)\n",
    "print(value/value2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im(x, y):\n",
    "  entropia_x = entropy(x)\n",
    "  print('entropia_x', entropia_x)\n",
    "  entropia_y = entropy(y)\n",
    "  entropy_x_y = joint_entropy(x, y)\n",
    "  return entropia_x + entropia_y - entropy_x_y\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(series) for count in counts.values() <generator object entropy.<locals>.<genexpr> at 0x16171ce40>\n",
      "-5.693640348287621\n"
     ]
    }
   ],
   "source": [
    "value = entropy(x)\n",
    "value2 = joint_entropy(x,y)\n",
    "\n",
    "print(value-value2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
